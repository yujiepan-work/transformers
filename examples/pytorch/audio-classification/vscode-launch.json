{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "[dev] w2v2 keyword spotting", 
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "3",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                // "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "dev-w2v2opt",
                "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/audio-classification",
            "program": "run_audio_classification.py",
            "args": [
                "--model_name_or_path", 
                // "/data/vchua/run/temp/wav2vec2-base-ft-keyword-spotting",
                "anton-l/wav2vec2-base-ft-keyword-spotting",
                "--dataset_name", "superb",
                "--dataset_config_name", "ks",
                "--output_dir", "/tmp/vscode-dev/dev-w2v2-keyword-qat",
                "--overwrite_output_dir",
                "--remove_unused_columns", "False",
                "--do_eval",
                "--do_train",
                "--nncf_config", "${workspaceFolder}/transformers/examples/pytorch/audio-classification/cfg-nncf/dev_nncf_wav2vec2_config.json",
                "--learning_rate", "3e-5",
                "--max_length_seconds", "1",
                "--attention_mask", "False",
                "--warmup_ratio", "0.1",
                "--num_train_epochs", "1",
                "--per_device_train_batch_size", "32",
                "--gradient_accumulation_steps", "4",
                "--per_device_eval_batch_size", "32",
                "--dataloader_num_workers", "4",
                "--logging_strategy", "steps",
                "--logging_steps", "1",
                "--evaluation_strategy", "epoch",
                "--save_strategy", "epoch",
                "--load_best_model_at_end", "True",
                "--metric_for_best_model", "accuracy",
                "--save_total_limit", "3",
                "--seed", "0",
                "--run_name", "w2v2-ac-qat",
                "--max_steps", "15",
                // "--teacher", "anton-l/wav2vec2-base-ft-keyword-spotting",
                // "--teacher_ratio", "0.9",
                // "--lr_scheduler_type", "cosine_with_restarts"
            ]
        },
        {
            "name": "[QAT] w2v2 keyword spotting", 
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                // "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "dev-w2v2opt",
                "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/audio-classification",
            "program": "run_audio_classification.py",
            "args": [
                "--model_name_or_path", "anton-l/wav2vec2-base-ft-keyword-spotting",
                "--dataset_name", "superb",
                "--dataset_config_name", "ks",
                "--output_dir", "/tmp/vscode-dev/w2v2-keyword-qat",
                "--overwrite_output_dir",
                "--remove_unused_columns", "False",
                "--do_eval",
                "--do_train",
                "--nncf_config", "${workspaceFolder}/optimum-openvino/optimum/intel/nncf/configs/nncf_wav2vec2_config.json",
                "--learning_rate", "3e-5",
                "--max_length_seconds", "1",
                "--attention_mask", "False",
                "--warmup_ratio", "0.1",
                "--num_train_epochs", "5",
                "--per_device_train_batch_size", "32",
                "--gradient_accumulation_steps", "4",
                "--per_device_eval_batch_size", "32",
                "--dataloader_num_workers", "4",
                "--logging_strategy", "steps",
                "--logging_steps", "1",
                "--evaluation_strategy", "epoch",
                "--save_strategy", "epoch",
                "--load_best_model_at_end", "True",
                "--metric_for_best_model", "accuracy",
                "--save_total_limit", "3",
                "--seed", "0",
                "--run_name", "w2v2-ac-qat",
                // "--max_steps", "5",
            ]
        },
        {
            "name": "[Eval] w2v2 keyword spotting", 
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "summarization",
                "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/audio-classification",
            "program": "run_audio_classification.py",
            "args": [
                "--model_name_or_path", "anton-l/wav2vec2-base-ft-keyword-spotting",
                "--dataset_name", "superb",
                "--dataset_config_name", "ks",
                "--output_dir", "/tmp/vscode-dev/w2v2-keyword",
                "--overwrite_output_dir",
                "--remove_unused_columns", "False",
                "--do_eval",
                "--max_length_seconds", "1",
                "--attention_mask", "False",
                "--per_device_eval_batch_size", "32",
                "--dataloader_num_workers", "4",
                "--seed", "0"
            ]
        },
    ]
}